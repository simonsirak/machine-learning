# Unsupervised Learning

Clustering is a way to group data into clusters. 

## K-Means Clustering
K-Means clustering is a clustering algorithm that favors spherical clusters, where the size of clusters are based on the distances 
between them.

K-means essentially picks K random points and label them as part of unique clusters. Then, it repeatedly calculates the mean of each 
cluster, and iterates through all points to cluster them with the closest mean, based on the *euclidean distance*. The mean is updated 
once per scan through all points.

K-means is guaranteed to converge, but not guaranteed to find an optimal solution, i.e a solution that reduces variance optimatlly
(it is a heuristic). The choice of K can be done based on the variance dropoff. Look for an elbow in the K-to-variance graph.
You may need to repeat the procedure some times to reduce chances of getting a poor clustering due to "starting out poorly". Depending 
on how many clusters there actually exists, the number of repeated executions you need to do will vary. But not by much; around 20 
repetitions should be well enough for K = 5-10 (I am basing this on the probability of choosing K points that are all in a unique cluster,
raised to the number of repetitions).

K-means is horrible at clustering e.g non-spherical classes, like classes that can be linearly separated. This is because the clusters 
are not based on euclidean distances, i.e they are not really "clusters".

Instead of trying to fit K-means, we could instead try to fit a mixture of K distributions, i.e a linear combination of K distributions,
<img src="https://render.githubusercontent.com/render/math?math=Pr(x|\theta)=\sum_{k=1}^{K}\pi_kPr(x|\theta_k)">
where <img src="https://render.githubusercontent.com/render/math?math=\theta=\{\pi_1,...,\pi_K,\theta_1,...,\theta_K\}">. 
However, the problem is that we do not know which point was generated by which component of the mixture, so we cannot optimize this 
probability. 

This can be solved using Expectation Maximization.

## Expectation Maximization (EM)
EM is a way to fit model parameters with missing (latent) variables. An example of this is the use of EM in HMM:s, where the hidden state 
is unknown. Another use-case is in clustering, specifically to address the problem above with fitting a mixture of K distributions. We 
can add a latent/hidden variable that assigns a datapoint to a component of the linear combination of distributions. You then optimize 
likelihood with respect to both latent and available variables, i.e optimize
<img src="https://render.githubusercontent.com/render/math?math=Pr(x_1,...,x_N,h_1,...,h_N|\theta)">.

### Description of EM Algorithm
TODO

### Evaluation of EM
It is similar to K-means in that it is guaranteed to find local maximum of the likelihood, and it is sensitive to initial conditions. 
However it can model clusters of many shapes, can include the use of priors (for regularization) and all datapoints are smoothly used 
to update all parameters (and not like in K-means where a point impacts 1 parameter at a time).
